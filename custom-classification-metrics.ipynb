{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4edd4-b393-4ce7-bc70-4bd9d0dafd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Check if y_true is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.to_numpy()  # Convert y_true to a NumPy array\n",
    "\n",
    "    # Check if y_pred is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.to_numpy()  # Convert y_pred to a NumPy array\n",
    "\n",
    "    # Check if y_true is already flattened (1D)\n",
    "    if y_true.ndim > 1:\n",
    "        y_true = y_true.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Check if y_pred is already flattened (1D)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Find unique class names in y_true and determine the number of unique classes\n",
    "    class_names = np.unique(y_true)  # Get unique class names from y_true\n",
    "    unique_classes = class_names.size  # Count the number of unique classes\n",
    "\n",
    "    # Initialize a confusion matrix with zeros, sized based on the number of unique classes\n",
    "    confusion_matrix = np.zeros((unique_classes, unique_classes), dtype=int)\n",
    "\n",
    "    # Map class names to indices for easy lookup\n",
    "    class_name_to_index = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Count occurrences of actual vs predicted labels\n",
    "    for actual, predicted in zip(y_true, y_pred):\n",
    "        # Ensure actual and predicted are scalar values, not arrays\n",
    "        actual = actual.item() if isinstance(actual, np.ndarray) else actual\n",
    "        predicted = predicted.item() if isinstance(predicted, np.ndarray) else predicted\n",
    "        \n",
    "        # Find the index for the actual and predicted class\n",
    "        actual_index = class_name_to_index[actual]\n",
    "        predicted_index = class_name_to_index[predicted]\n",
    "\n",
    "        # Increment the appropriate cell in the confusion matrix\n",
    "        confusion_matrix[actual_index, predicted_index] += 1\n",
    "\n",
    "    # Print the confusion matrix row by row\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    for row in confusion_matrix:\n",
    "        print(\" \".join(map(str, row)))  # Print each row of the confusion matrix\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Accuracy Calculation\n",
    "    \n",
    "    # Sum of the diagonal elements (correct predictions)\n",
    "    correct_predictions = np.trace(confusion_matrix)  # np.trace() gives the sum of diagonal elements\n",
    "\n",
    "    # Total number of predictions (sum of all elements in the matrix)\n",
    "    total_predictions = np.sum(confusion_matrix)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "        \n",
    "    # Precision Calculation\n",
    "\n",
    "    def calculate_precision(confusion_matrix, class_names):\n",
    "        precision = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its precision\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Positive (FP) is the sum of the column (excluding the diagonal)\n",
    "            false_positive = np.sum(confusion_matrix[:, i]) - true_positive\n",
    "            \n",
    "            # Precision for the current class\n",
    "            precision[class_name] = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0        \n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    precision = calculate_precision(confusion_matrix, class_names)\n",
    "    print(\"\\nPrecision for each class:\")\n",
    "    for class_name, precision_value in precision.items():\n",
    "        print(f\"{class_name}: {precision_value:}\")\n",
    "\n",
    "    total_precision = sum(precision.values())\n",
    "    print(f\"\\nMacro precision: {total_precision / unique_classes}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Recall Calculation\n",
    "    \n",
    "    def calculate_recall(confusion_matrix, class_names):\n",
    "        recall = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its recall\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Negative (FN) is the sum of the row (excluding the diagonal)\n",
    "            false_negative = np.sum(confusion_matrix[i, :]) - true_positive\n",
    "            \n",
    "            # Recall for the current class\n",
    "            recall[class_name] = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0        \n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    recall = calculate_recall(confusion_matrix, class_names)\n",
    "    print(\"\\nRecall for each class:\")\n",
    "    for class_name, recall_value in recall.items():\n",
    "        print(f\"{class_name}: {recall_value:.4f}\")\n",
    "\n",
    "    total_recall = sum(recall.values())\n",
    "    print(f\"\\nMacro recall: {total_recall / unique_classes}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "\n",
    "    # F1 Score Calculation\n",
    "    \n",
    "    def calculate_f1_score(precision, recall):\n",
    "        f1_scores = {}\n",
    "    \n",
    "        # Calculate F1 score for each class\n",
    "        for class_name in precision.keys():\n",
    "            p = precision[class_name]\n",
    "            r = recall[class_name]\n",
    "            \n",
    "            # Calculate F1 score for the class, handling cases where p + r = 0\n",
    "            f1_scores[class_name] = (2 * p * r) / (p + r) if (p + r) != 0 else 0\n",
    "        \n",
    "        return f1_scores\n",
    "    \n",
    "    f1_scores = calculate_f1_score(precision, recall)\n",
    "    print(\"\\nF1 Score for each class:\")\n",
    "    for class_name, f1_value in f1_scores.items():\n",
    "        print(f\"{class_name}: {f1_value:}\")\n",
    "        \n",
    "    # Macro F1 Score Calculation\n",
    "    total_f1 = sum(f1_scores.values())  # Sum of F1 scores for each class\n",
    "    macro_f1 = total_f1 / len(f1_scores)  # Average F1 score across all classes\n",
    "    \n",
    "    print(f\"\\nMacro F1 score: {macro_f1:}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
